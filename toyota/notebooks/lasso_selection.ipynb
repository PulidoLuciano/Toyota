{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, LassoCV, lasso_path\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from toyota.utils import load_dataset, get_metrics, LinearRegDiagnostic\n",
    "\n",
    "toyota = cut_outliers.copy()\n",
    "\n",
    "def lasso_selection(X_train, X_test, y_train, y_test, min_alpha=0, max_alpha=4, steps=100):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    alphas = np.logspace(min_alpha, max_alpha, steps)\n",
    "    coefs = []\n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "\n",
    "    for a in alphas:\n",
    "        clf = Lasso(alpha=a, max_iter=10000)\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        coefs.append(clf.coef_)\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse_list.append(np.sqrt(mse))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_list.append(r2)\n",
    "\n",
    "    alphas_idx = pd.Index(alphas, name=\"alpha\")\n",
    "    coefs_df = pd.DataFrame(coefs, index=alphas_idx, columns=X_train.columns)\n",
    "    coefs_plot(coefs_df)\n",
    "    rmse_plot(alphas, rmse_list)\n",
    "    r2_plot(alphas, r2_list)\n",
    "    ordered_coefs = print_coefs(coefs_df, rmse_list)\n",
    "    worst_coefs = ordered_coefs[ordered_coefs == 0].index.tolist()\n",
    "    return worst_coefs\n",
    "\n",
    "def coefs_plot(coefs):\n",
    "    sns.lineplot(data=coefs)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.ylabel(\"Coeficiente\")\n",
    "    plt.title(\"Trayectoria de coeficientes Lasso\")\n",
    "    plt.show()\n",
    "\n",
    "def rmse_plot(alphas, rmse_list):\n",
    "    sns.lineplot(x=alphas, y=rmse_list)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    min_rmse_idx = np.argmin(rmse_list)\n",
    "    min_alpha = alphas[min_rmse_idx]\n",
    "    min_rmse = rmse_list[min_rmse_idx]\n",
    "    plt.plot(min_alpha, min_rmse, 'ro', label=f'Min RMSE: {min_rmse:.2f}\\nAlpha: {min_alpha:.2e}')\n",
    "    plt.legend()\n",
    "    plt.title(\"RMSE vs alpha\")\n",
    "    plt.show()\n",
    "\n",
    "def r2_plot(alphas, r2_list):\n",
    "    sns.lineplot(x=alphas, y=r2_list)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel(\"R2\")\n",
    "    max_r2_idx = np.argmax(r2_list)\n",
    "    max_alpha = alphas[max_r2_idx]\n",
    "    max_r2 = r2_list[max_r2_idx]\n",
    "    plt.plot(max_alpha, max_r2, 'ro', label=f'Max R2: {max_r2:.2f}\\nAlpha: {max_alpha:.2e}')\n",
    "    plt.legend()\n",
    "    plt.title(\"R2 vs alpha\")\n",
    "    plt.show()\n",
    "\n",
    "def print_coefs(coefs, mse_list):\n",
    "    min_mse_idx = np.argmin(mse_list)\n",
    "    coefs_df = np.abs(coefs.iloc[min_mse_idx]).sort_values(ascending=False)\n",
    "    print(coefs_df)\n",
    "    return coefs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer la primera ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = toyota.drop(columns=[\"Price\"], axis=1)\n",
    "y = toyota[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "lasso_selection(X_train, X_test, y_train, y_test, min_alpha=-10, max_alpha=10, steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ajustar el minimo del alpha para reducir variables sin perder demasiada calidad en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_selection(X_train, X_test, y_train, y_test, min_alpha=0.6, max_alpha=4, steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo cuenta con muchas variables y parece empezar a empeorar rapidamente si seguimos aumentando el alpha minimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aqui hemos determinado el mejor modelo con: \n",
    "\n",
    "Features: 37:\n",
    "\n",
    "lista de features: [\n",
    "    \"Mfg_Year\",\n",
    "    \"KM\",\n",
    "    \"Weight\",\n",
    "    \"m_vvtli\",\n",
    "    \"Automatic_airco\",\n",
    "    \"Quarterly_Tax\",\n",
    "    \"m_d4d\",\n",
    "    \"CNG\",\n",
    "    \"Guarantee_Period\",\n",
    "    \"Diesel\",\n",
    "    \"m_liftb\",\n",
    "    \"HP\",\n",
    "    \"Automatic\",\n",
    "    \"m_hatch_b\",\n",
    "    \"m_g6\",\n",
    "    \"Powered_Windows\",\n",
    "    \"m_sedan\",\n",
    "    \"Mfr_Guarantee\",\n",
    "    \"CD_Player\",\n",
    "    \"Five_Doors\",\n",
    "    \"m_comfort\",\n",
    "    \"Airco\",\n",
    "    \"Boardcomputer\",\n",
    "    \"BOVAG_Guarantee\",\n",
    "    \"Trunk\",\n",
    "    \"Backseat_Divider\",\n",
    "    \"Sport_Model\",\n",
    "    \"Tow_Bar\",\n",
    "    \"ABS\",\n",
    "    \"Airbag_2\",\n",
    "    \"Met_Color\",\n",
    "    \"m_matic4\",\n",
    "    \"Power_Steering\",\n",
    "    \"Radio\",\n",
    "    \"m_luna\",\n",
    "    \"m_terra\",\n",
    "    \"m_16v\"\n",
    "]\n",
    "\n",
    "r2 = 0.91\n",
    "rmse = 909.04\n",
    "alpha = 3.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternativas con limpieza de outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, veamos alternativas de modelos que podrían resultar viables, pero con limpieza de outliers. Esta vez, asignaremos un margen medido en \"desviaciones estandar\" del error estandar y tomando las 3 mejores opciones de lambda maximo, ademas del lambda minimo, cuyos estadisticos del modelo se aproximen al mejor obtenido hasta ahora. Al final obtenemos la lista de indices de los outliers que pueden quitarse para mejorar el modelo de lasso. Usamos LassoCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la funcion que busca el rango de alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_with_threshold(X, y, no_deviations=1):\n",
    "    # Es decir, alpha entre exp(1) y exp(8)\n",
    "    alphas_grid = np.logspace(-3, 6, 400)\n",
    "\n",
    "    # Ajustar LassoCV con el rango de alphas adecuado\n",
    "    lasso_cv = LassoCV(alphas=alphas_grid, cv=10, random_state=42)\n",
    "    lasso_cv.fit(X, y)\n",
    "\n",
    "    # Obtener alphas y errores\n",
    "    alphas = lasso_cv.alphas_\n",
    "    mse_path = lasso_cv.mse_path_\n",
    "    mean_mse = np.mean(mse_path, axis=1)\n",
    "    std_mse = np.std(mse_path, axis=1)\n",
    "\n",
    "    # Índices y valores de alpha mínimo y 1-SE\n",
    "    idx_min = np.argmin(mean_mse)\n",
    "    alpha_min = alphas[idx_min]\n",
    "    \n",
    "    se_min = std_mse[idx_min]\n",
    "    threshold = mean_mse[idx_min] + no_deviations * se_min  # 1 std deviation above the minimum MSE\n",
    "\n",
    "    # Find the first alpha whose mean MSE is within 1 std deviation of the minimum\n",
    "    idxs_se = np.where(mean_mse <= threshold)[0]\n",
    "    if len(idxs_se) > 0:\n",
    "        # Ensure alpha_se is the first such alpha (i.e., the largest/most regularized within the threshold)\n",
    "        idx_se = idxs_se[0]\n",
    "        idx_2_se = idxs_se[1]\n",
    "        idx_3_se = idxs_se[2]\n",
    "    else:\n",
    "        # Fallback: use the minimum MSE index if none found (should not happen)\n",
    "        idx_se = idx_min\n",
    "        idx_2_se = idx_min\n",
    "        idx_3_se = idx_min\n",
    "\n",
    "    alpha_1 = alphas[idx_se]\n",
    "    alpha_2 = alphas[idx_2_se]\n",
    "    alpha_3 = alphas[idx_3_se]\n",
    "    \n",
    "    # Ruta de coeficientes para todas las alphas\n",
    "    alphas_path, coefs_path, _ = lasso_path(X, y, alphas=alphas)\n",
    "    n_features = (coefs_path != 0).sum(axis=0)\n",
    "    # Crear gráfico con eje superior\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Curva de MSE con error estándar\n",
    "    ax.errorbar(np.log(alphas), mean_mse, yerr=std_mse, fmt='-o', capsize=3, label='CV Error ± 1 SE')\n",
    "\n",
    "    # Líneas verticales para alpha mínimo y 1-SE\n",
    "    ax.axvline(np.log(alpha_min), color='r', linestyle='--', linewidth=2,\n",
    "            label=f'min alpha ({n_features[idx_min]} features)')\n",
    "    ax.axvline(np.log(alpha_1), color='g', linestyle=':', linewidth=2,\n",
    "            label=f'1-SE alpha ({n_features[idx_se]} features)')\n",
    "    ax.axvline(np.log(alpha_2), color='b', linestyle=':', linewidth=2,\n",
    "            label=f'1-SE alpha ({n_features[idx_2_se]} features)')\n",
    "    ax.axvline(np.log(alpha_3), color='y', linestyle=':', linewidth=2,\n",
    "            label=f'1-SE alpha ({n_features[idx_3_se]} features)')\n",
    "    \n",
    "    # Etiquetas normales\n",
    "    ax.set_xlabel('log(alpha)')\n",
    "    ax.set_ylabel('MSE de validación cruzada')\n",
    "    ax.set_title('Validación cruzada para Lasso')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Eje superior con número de features\n",
    "    ax_top = ax.twiny()\n",
    "    ax_top.set_xlim(ax.get_xlim())  # Sincronizar límites con eje inferior\n",
    "    ax_top.set_xticks(np.log(alphas[::2]))  # Usar cada 2 alphas para no sobrecargar\n",
    "    ax_top.set_xticklabels(n_features[::2])\n",
    "    ax_top.set_xlabel('Número de features seleccionadas')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Obtener y mostrar los nombres de las features seleccionadas\n",
    "    selected_min = X.columns[coefs_path[:, idx_min] != 0].tolist()\n",
    "    selected_1 = X.columns[coefs_path[:, idx_se] != 0].tolist()\n",
    "    selected_2 = X.columns[coefs_path[:, idx_2_se] != 0].tolist()\n",
    "    selected_3 = X.columns[coefs_path[:, idx_3_se] != 0].tolist()\n",
    "    \n",
    "    \n",
    "    # Imprimir de menor a mayor lambda..\n",
    "    print(f\"\\nLambda minimo (base): {alpha_min} (log10={np.log10(alpha_min):.2f})\")\n",
    "    print(f\"Features:\")\n",
    "    print(selected_min)\n",
    "    \n",
    "    print(f\"\\nTercera mejor opcion de lambra maximo: {alpha_3} (log10={np.log10(alpha_3):.2f})\")\n",
    "    print(f\"Features:\")\n",
    "    print(selected_3)\n",
    "    \n",
    "    print(f\"\\nSegunda mejor opcion de lambra maximo: {alpha_2} (log10={np.log10(alpha_2):.2f})\")\n",
    "    print(f\"Features:\")\n",
    "    print(selected_2)\n",
    "    \n",
    "    print(f\"\\nPrimera mejor opcion de lambda maximo: {alpha_1} (log10={np.log10(alpha_1):.2f})\")\n",
    "    print(f\"Features:\")\n",
    "    print(selected_1)\n",
    "    \n",
    "    \n",
    "    return [\n",
    "        (alpha_min, selected_min),\n",
    "        (alpha_3, selected_3),\n",
    "        (alpha_2, selected_2),\n",
    "        (alpha_1, selected_1),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la funcion simple de lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = toyota.drop(columns=[\"Price\"], axis=1)\n",
    "y = toyota[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lasso_simple(X, y, features, alpha):\n",
    "\n",
    "    X = X[features]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "    clf = Lasso()\n",
    "    # Train the model with different regularisation strengths\n",
    "    clf.set_params(alpha=alpha).fit(X_train, y_train)\n",
    "    coef = clf.coef_\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Features: \", len(features))\n",
    "    print(\"RMSE: \",rmse)\n",
    "    print(\"R²: \",r2)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos 2 opciones de desviaciones estandar ya que son las mas proximas a los mejores modelos obtenidos anteriormente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero probamos con una tolerancia de 0.35 desviaciones estandar para el error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list_1 = lasso_with_threshold(X_train, y_train, no_deviations=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in res_list_1:\n",
    "    lasso_simple(X, y, features=res[1], alpha=res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo probamos una tolerancia de 0.4 desviaciones para que pueda reducir mas el numero de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list_2 = lasso_with_threshold(X_train, y_train, no_deviations=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in res_list_2:\n",
    "    lasso_simple(X, y, features=res[1], alpha=res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos si eliminando outliers podemos hacer algo con los mejores modelos obtenidos hasta el momento. Notamos que llega un punto en el que si seguimos sacando outliers el modelo comienza a empeorar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CASO 1 (Busqueda manual)\n",
    "*Features:  37*\n",
    "\n",
    "*RMSE:  909.04*\n",
    "\n",
    "*R²:  0.91*\n",
    "\n",
    "*Lambda = 3.98*\n",
    "\n",
    "*Features: [ \"Mfg_Year\", \"KM\", \"Weight\", \"m_vvtli\", \"Automatic_airco\", \"Quarterly_Tax\", \"m_d4d\", \"CNG\", \"Guarantee_Period\", \"Diesel\", \"m_liftb\",    \"HP\",    \"Automatic\",    \"m_hatch_b\",    \"m_g6\",    \"Powered_Windows\",    \"m_sedan\", \"Mfr_Guarantee\",    \"CD_Player\",    \"Five_Doors\",    \"m_comfort\",    \"Airco\",    \"Boardcomputer\",    \"BOVAG_Guarantee\",    \"Trunk\", \"Backseat_Divider\",    \"Sport_Model\",    \"Tow_Bar\",    \"ABS\",    \"Airbag_2\",    \"Met_Color\",    \"m_matic4\",    \"Power_Steering\",\n",
    "\"Radio\",    \"m_luna\",    \"m_terra\",    \"m_16v\"]*\n",
    "\n",
    "#### CASO 2 (Busqueda con threshold)\n",
    "*Features:  36*\n",
    "\n",
    "*RMSE:  925.6536556988251*\n",
    "\n",
    "*R²:  0.902661510112417*\n",
    "\n",
    "*Lambda = 3.66*\n",
    "\n",
    "*Features: ['Mfg_Year', 'KM', 'HP', 'Met_Color', 'Automatic', 'cc', 'Gears', 'Quarterly_Tax', 'Weight', 'Mfr_Guarantee', 'BOVAG_Guarantee', 'Guarantee_Period', 'ABS', 'Airbag_1', 'Airbag_2', 'Airco', 'Automatic_airco', 'Boardcomputer', 'CD_Player', 'Powered_Windows', 'Power_Steering', 'Metallic_Rim', 'Tow_Bar', 'CNG', 'm_16v', 'm_terra', 'm_liftb', 'm_wagon', 'm_sol', 'm_sedan', 'm_comfort', 'm_g6', 'm_d4d', 'm_gtsi', 'Trunk', 'Five_Doors']*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la funcion que nos determina la lista de outliers basado en la performance del Modelo de Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificador de outliers\n",
    "def lasso_clean_outliers(selected_columns, alpha, outliers):\n",
    "    print(\"Antes: \", toyota.shape)\n",
    "    df_2 = toyota.drop(outliers, axis=0)\n",
    "    print(\"Despues: \", df_2.shape)\n",
    "    \n",
    "    X = df_2.drop(columns=[\"Price\"], axis=1)\n",
    "    y = df_2[\"Price\"]\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "    model = lasso_simple(X, y, features=selected_columns, alpha=alpha)\n",
    "\n",
    "    from toyota.utils_lasso import SklearnLassoDiagnostic\n",
    "    diagnosticPlotter = SklearnLassoDiagnostic(model, X[selected_columns], y, selected_columns)\n",
    "    diagnosticPlotter()\n",
    "\n",
    "    # Convert each outlier list (list of numpy arrays) to a DataFrame\n",
    "    outlier_dfs = [\n",
    "        pd.DataFrame(diagnosticPlotter.residuals_vs_fitted_outliers),\n",
    "        pd.DataFrame(diagnosticPlotter.qq_plot_outliers),\n",
    "        pd.DataFrame(diagnosticPlotter.scale_location_outliers),\n",
    "        pd.DataFrame(diagnosticPlotter.leverage_plot_outliers)\n",
    "    ]\n",
    "\n",
    "    # Armamos una lista con todos los valores únicos de outlier_dfs\n",
    "    all_outlier_values = []\n",
    "    for df in outlier_dfs:\n",
    "        all_outlier_values.extend(df.values.flatten())\n",
    "    # Eliminamos duplicados y NaNs, y convertimos a enteros si corresponde\n",
    "    all_outlier_values = [int(x) for x in set(all_outlier_values) if not pd.isnull(x)]\n",
    "\n",
    "    # Para cada idx en all_outlier_values, buscamos el registro en X y luego su índice en toyota (sin \"Price\")\n",
    "    toyota_no_price = toyota.drop(columns=[\"Price\"], errors=\"ignore\")\n",
    "\n",
    "    global_indexes = []\n",
    "    for idx in all_outlier_values:\n",
    "        row = X[selected_columns].iloc[idx]\n",
    "        # Buscamos la(s) fila(s) en toyota_no_price que coincidan exactamente con row\n",
    "        mask = (toyota_no_price[selected_columns] == row.values).all(axis=1)\n",
    "        # matching_indexes contiene los índices de toyota_no_price donde hay coincidencia exacta\n",
    "        matching_indexes = toyota_no_price.index[mask].tolist()\n",
    "        global_indexes.extend(matching_indexes)\n",
    "        \n",
    "    # Eliminamos duplicados\n",
    "    global_indexes = list(set(global_indexes))\n",
    "    print(\"Índices globales correspondientes a los outliers:\", global_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos si mejora con la limpieza..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Caso 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Mfg_Year\", \"KM\", \"Weight\", \"m_vvtli\", \"Automatic_airco\", \"Quarterly_Tax\", \"m_d4d\", \"CNG\", \"Guarantee_Period\", \"Diesel\", \"m_liftb\",    \"HP\",    \"Automatic\",    \"m_hatch_b\",    \"m_g6\",    \"Powered_Windows\",    \"m_sedan\", \"Mfr_Guarantee\",    \"CD_Player\",    \"Five_Doors\",    \"m_comfort\",    \"Airco\",    \"Boardcomputer\",    \"BOVAG_Guarantee\",    \"Trunk\", \"Backseat_Divider\",    \"Sport_Model\",    \"Tow_Bar\",    \"ABS\",    \"Airbag_2\",    \"Met_Color\",    \"m_matic4\",    \"Power_Steering\",\n",
    "\"Radio\",    \"m_luna\",    \"m_terra\",    \"m_16v\"]\n",
    "outliers = [1261, 14, 16, 49, 1432]\n",
    "lasso_clean_outliers(features, alpha=3.98, outliers=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que empeora eliminando los primeros outliers encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Caso 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Mfg_Year', 'KM', 'HP', 'Met_Color', 'Automatic', 'cc', 'Gears', 'Quarterly_Tax', 'Weight', 'Mfr_Guarantee', 'BOVAG_Guarantee', 'Guarantee_Period', 'ABS', 'Airbag_1', 'Airbag_2', 'Airco', 'Automatic_airco', 'Boardcomputer', 'CD_Player', 'Powered_Windows', 'Power_Steering', 'Metallic_Rim', 'Tow_Bar', 'CNG', 'm_16v', 'm_terra', 'm_liftb', 'm_wagon', 'm_sol', 'm_sedan', 'm_comfort', 'm_g6', 'm_d4d', 'm_gtsi', 'Trunk', 'Five_Doors']\n",
    "outliers = [\n",
    "    16, 49, 14, 15, 330, 1133, 1261, 17, 1432, \n",
    "    410, 320, 1378, 388, 1131, 94, 991, 609,\n",
    "    5, 102, 1072, 1268, 314, 1313, 163, 1353, \n",
    "    106, 19, 91, 64, 4, 114, 826, 668,\n",
    "    # Si seguimos limpiando, Lasso no mejora.\n",
    "]\n",
    "lasso_clean_outliers(features, alpha=3.66, outliers=outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este nos permite limpiar el dataset y mejorar sustancialmente los estadisticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conclusion*: Hemos intentado mejorar 2 de los mejores modelos que teniamos hasta el momento con lasso, uno de 36 features y otro de 37 features. Pero al momento de borrar outliers, el modelo que mejoraba mas con la eliminacion de outliers es el de 36 variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toyota",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
