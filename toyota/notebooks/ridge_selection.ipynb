{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "toyota = cut_outliers.copy()\n",
    "\n",
    "def ridge_selection(X_train, X_test, y_train, y_test, min_alpha=0, max_alpha=4, steps=100, n_worst=5):\n",
    "    clf = Ridge()\n",
    "\n",
    "    # Generate values for `alpha` that are evenly distributed on a logarithmic scale\n",
    "    alphas = np.logspace(min_alpha, max_alpha, steps)\n",
    "    coefs = []\n",
    "    rmse_list = []\n",
    "    r2_list = []\n",
    "\n",
    "    # Train the model with different regularisation strengths\n",
    "    for a in alphas:\n",
    "        clf.set_params(alpha=a).fit(X_train, y_train)\n",
    "        coefs.append(clf.coef_)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse_list.append(np.sqrt(mse))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        r2_list.append(r2)\n",
    "\n",
    "    alphas = pd.Index(alphas, name=\"alpha\")\n",
    "    coefs = pd.DataFrame(coefs, index=alphas, columns=[f\"{name}\" for _, name in enumerate(X_train.columns)])\n",
    "    coefs_plot(coefs)\n",
    "    rmse_plot(alphas, rmse_list)\n",
    "    r2_plot(alphas, r2_list)\n",
    "    worst_coefs = print_coefs(coefs, rmse_list).tail(n_worst).index.tolist()\n",
    "    return worst_coefs\n",
    "\n",
    "def coefs_plot(coefs):\n",
    "    sns.lineplot(data=coefs)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "def rmse_plot(alphas, rmse_list):\n",
    "    sns.lineplot(x=alphas, y=rmse_list)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    # Find and mark the minimum MSE point\n",
    "    min_mse_idx = np.argmin(rmse_list)\n",
    "    min_alpha = alphas[min_mse_idx]\n",
    "    min_mse = rmse_list[min_mse_idx]\n",
    "    plt.plot(min_alpha, min_mse, 'ro', label=f'Min RMSE: {min_mse:.2f}\\nAlpha: {min_alpha:.2e}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def r2_plot(alphas, r2_list):\n",
    "    sns.lineplot(x=alphas, y=r2_list)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.ylabel(\"R2\")\n",
    "    min_r2_idx = np.argmax(r2_list)\n",
    "    min_alpha = alphas[min_r2_idx]\n",
    "    min_r2 = r2_list[min_r2_idx]\n",
    "    plt.plot(min_alpha, min_r2, 'ro', label=f'Min R2: {min_r2:.2f}\\nAlpha: {min_alpha:.2e}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def print_coefs(coefs, mse_list):\n",
    "    min_mse_idx = np.argmin(mse_list)\n",
    "    coefs_df = np.abs(coefs.iloc[min_mse_idx]).sort_values(ascending=False)\n",
    "    print(coefs_df)\n",
    "    return coefs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer la primera ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = toyota.drop(columns=[\"Price\"], axis=1)\n",
    "y = toyota[\"Price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "ridge_selection(X_train, X_test, y_train, y_test, min_alpha=-10, max_alpha=10, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustamos el logspace de prueba, queremos un valor de alpha mas alto que nos permita sacar mas variables sin empeorar demasiado el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_selection(X_train, X_test, y_train, y_test, min_alpha=0, max_alpha=5, steps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queremos iterar multiples veces eliminando las peores variables en cada una. Vamos a eliminar en cada iteracion las peores cinco variables y corroborar los mejores resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ajustar alpha a 0 como minimo para el logspace. Fue uno de los mejores valores probados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_ridge_selection(df, min_alpha=-10, max_alpha=10, steps=100, drop_coefs=5):\n",
    "    X = df.drop(columns=[\"Price\"], axis=1)\n",
    "    y = df[\"Price\"]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "    while len(X.columns) > drop_coefs:\n",
    "        worst_coefs = ridge_selection(X_train, X_test, y_train, y_test, min_alpha, max_alpha, steps, n_worst=drop_coefs)\n",
    "        X = X.drop(columns=worst_coefs, axis=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "iterate_ridge_selection(toyota, min_alpha=0, max_alpha=4, steps=100, drop_coefs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver se puede quitar variables sin que afecte demasiado al modelo. Por el criterio de Navaja de Ockham queremos quedarnos con el modelo más simple. Vamos a probar iterando desde el modelo de 15 variables y sacando de a una variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifteen_df = toyota.drop(columns=['m_exec', 'Metallic_Rim', 'Sport_Model', 'm_vvti', 'm_16v', 'Radio', 'Met_Color',\n",
    "                              'Central_Lock', 'm_luna', 'm_terra', 'm_sport', 'Airbag_2', 'Airbag_1', 'ABS', 'Tow_Bar',\n",
    "                              'Automatic', 'm_wagon', 'Mistlamps', 'm_sedan', 'BOVAG_Guarantee', 'Backseat_Divider',\n",
    "                              'm_sol', 'Airco', 'Power_Steering', 'cc',\n",
    "                              'Boardcomputer', 'CD_Player', 'Gears'], axis=1)\n",
    "iterate_ridge_selection(fifteen_df, min_alpha=0, max_alpha=4, steps=20, drop_coefs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tan solo en la primera iteracion el modelo ya empeora el RMSE en 30 unidades. Consideramos que lo ideal puede ser quedarse con las 15 features, considerando el aumento de RMSE al eliminar mas.\n",
    "\n",
    "Mfg_Year            7799.164414\n",
    "Weight              4232.299238\n",
    "KM                  4193.445233\n",
    "m_vvtli             2728.438711\n",
    "Quarterly_Tax       2387.143645\n",
    "Automatic_airco     2106.063777\n",
    "CNG                 1824.149363\n",
    "Diesel              1678.914642\n",
    "m_d4d               1252.832214\n",
    "Guarantee_Period     986.240697\n",
    "m_matic3             711.423817\n",
    "m_gtsi               586.591210\n",
    "m_g6                 561.510540\n",
    "m_liftb              536.806869\n",
    "m_bns                514.864141\n",
    "Powered_Windows      474.923391\n",
    "m_hatch_b            415.774423\n",
    "m_matic4             415.249804\n",
    "m_comfort            412.660478\n",
    "Mfr_Guarantee        363.974532\n",
    "HP                   133.164061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_simple(X, y, features, alpha):\n",
    "    X = X[features]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "    \n",
    "    clf = Ridge()\n",
    "    \n",
    "    clf.set_params(alpha=alpha).fit(X_train, y_train)\n",
    "    coef = clf.coef_\n",
    "    y_pred = clf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"Features: \", len(features))\n",
    "    print(\"RMSE: \",rmse)\n",
    "    print(\"R²: \",r2)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos si podemos mejorar limpiando outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificador de outliers\n",
    "def ridge_clean_outliers(selected_columns, alpha, outliers):\n",
    "    print(\"Antes: \", toyota.shape)\n",
    "    df_2 = toyota.drop(outliers, axis=0)\n",
    "    print(\"Despues: \", df_2.shape)\n",
    "    \n",
    "    X = df_2.drop(columns=[\"Price\"], axis=1)\n",
    "    y = df_2[\"Price\"]\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "    model = ridge_simple(X, y, features=selected_columns, alpha=alpha)\n",
    "\n",
    "    from toyota.utils_ridge import SklearnRidgeDiagnostic\n",
    "    diagnosticPlotter = SklearnRidgeDiagnostic(model, X[selected_columns], y, selected_columns)\n",
    "    diagnosticPlotter()\n",
    "\n",
    "    # Convert each outlier list (list of numpy arrays) to a DataFrame\n",
    "    outlier_dfs = [\n",
    "        pd.DataFrame(diagnosticPlotter.residuals_vs_fitted_outliers),\n",
    "        pd.DataFrame(diagnosticPlotter.qq_plot_outliers),\n",
    "        pd.DataFrame(diagnosticPlotter.scale_location_outliers),\n",
    "        pd.DataFrame(diagnosticPlotter.leverage_plot_outliers)\n",
    "    ]\n",
    "\n",
    "    # Armamos una lista con todos los valores únicos de outlier_dfs\n",
    "    all_outlier_values = []\n",
    "    for df in outlier_dfs:\n",
    "        all_outlier_values.extend(df.values.flatten())\n",
    "    # Eliminamos duplicados y NaNs, y convertimos a enteros si corresponde\n",
    "    all_outlier_values = [int(x) for x in set(all_outlier_values) if not pd.isnull(x)]\n",
    "\n",
    "    # Para cada idx en all_outlier_values, buscamos el registro en X y luego su índice en toyota (sin \"Price\")\n",
    "    toyota_no_price = toyota.drop(columns=[\"Price\"], errors=\"ignore\")\n",
    "\n",
    "    global_indexes = []\n",
    "    for idx in all_outlier_values:\n",
    "        row = X[selected_columns].iloc[idx]\n",
    "        # Buscamos la(s) fila(s) en toyota_no_price que coincidan exactamente con row\n",
    "        mask = (toyota_no_price[selected_columns] == row.values).all(axis=1)\n",
    "        # matching_indexes contiene los índices de toyota_no_price donde hay coincidencia exacta\n",
    "        matching_indexes = toyota_no_price.index[mask].tolist()\n",
    "        global_indexes.extend(matching_indexes)\n",
    "        \n",
    "    # Eliminamos duplicados\n",
    "    global_indexes = list(set(global_indexes))\n",
    "    print(\"Índices globales correspondientes a los outliers:\", global_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Mfg_Year', 'Weight', 'KM', 'm_vvtli', 'Quarterly_Tax', 'Automatic_airco', 'CNG', 'Diesel', 'm_d4d', 'Guarantee_Period', 'm_matic3', 'm_gtsi', 'm_g6', 'm_liftb', 'm_bns', 'Powered_Windows', 'm_hatch_b', 'm_matic4', 'm_comfort', 'Mfr_Guarantee', 'HP']\n",
    "outliers = [1131, 1133] # mejora de -50 rmse y +0.0087 r2\n",
    "ridge_clean_outliers(features, 1, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metamos en Ridge las features que obtuvimos en el manual feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  ['Mfg_Year',\n",
    "'Weight',\n",
    "'m_vvtli',\n",
    "'KM',\n",
    "'Quarterly_Tax',\n",
    "'CNG',\n",
    "'Automatic_airco',\n",
    "'Guarantee_Period',\n",
    "'cc',\n",
    "'HP']\n",
    "outliers = []\n",
    "ridge_clean_outliers(features, 0.4, outliers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toyota",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
